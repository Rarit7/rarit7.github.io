<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="Deeplearning.ai网课《深度学习》第二章笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="吴恩达深度学习笔记（1）——超参数调试与优化">
<meta property="og:url" content="http://ster.im/ng_01/index.html">
<meta property="og:site_name" content="Rarit7&#39;s Blog">
<meta property="og:description" content="Deeplearning.ai网课《深度学习》第二章笔记">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2018-04-10T05:56:27.000Z">
<meta property="article:modified_time" content="2018-04-18T07:37:53.770Z">
<meta property="article:author" content="Rarit7">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="Andrew Ng">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon-192x192.png">
        
      
    
    <!-- title -->
    <title>吴恩达深度学习笔记（1）——超参数调试与优化</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 5.4.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" "Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="上一篇 " href="/ng_02/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="下一篇 " href="/kaggle_03/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="返回顶部 " href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="分享文章 " href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://ster.im/ng_01/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://ster.im/ng_01/&text=吴恩达深度学习笔记（1）——超参数调试与优化"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://ster.im/ng_01/&title=吴恩达深度学习笔记（1）——超参数调试与优化"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://ster.im/ng_01/&is_video=false&description=吴恩达深度学习笔记（1）——超参数调试与优化"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=吴恩达深度学习笔记（1）——超参数调试与优化&body=Check out this article: http://ster.im/ng_01/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://ster.im/ng_01/&title=吴恩达深度学习笔记（1）——超参数调试与优化"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://ster.im/ng_01/&title=吴恩达深度学习笔记（1）——超参数调试与优化"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://ster.im/ng_01/&title=吴恩达深度学习笔记（1）——超参数调试与优化"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://ster.im/ng_01/&title=吴恩达深度学习笔记（1）——超参数调试与优化"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://ster.im/ng_01/&name=吴恩达深度学习笔记（1）——超参数调试与优化&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://ster.im/ng_01/&t=吴恩达深度学习笔记（1）——超参数调试与优化"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2"><span class="toc-number">1.</span> <span class="toc-text">深度学习的实用层面</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86-%E5%BC%80%E5%8F%91%E9%9B%86-%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-number">1.1.</span> <span class="toc-text">训练集&#x2F;开发集&#x2F;测试集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="toc-number">1.2.</span> <span class="toc-text">偏差和方差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.</span> <span class="toc-text">训练神经网络的基本方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L-2-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">1.4.</span> <span class="toc-text">$L_2$正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">1.5.</span> <span class="toc-text">Dropout正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">1.6.</span> <span class="toc-text">其他正则化方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E8%BE%93%E5%85%A5"><span class="toc-number">1.7.</span> <span class="toc-text">归一化输入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="toc-number">1.8.</span> <span class="toc-text">梯度消失和梯度爆炸</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C"><span class="toc-number">1.9.</span> <span class="toc-text">梯度检验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96"><span class="toc-number">2.</span> <span class="toc-text">优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Mini-Batch%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.1.</span> <span class="toc-text">Mini-Batch梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87"><span class="toc-number">2.2.</span> <span class="toc-text">指数加权平均</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Momentum%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.3.</span> <span class="toc-text">Momentum梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RMSprop"><span class="toc-number">2.4.</span> <span class="toc-text">RMSprop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adam%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">2.5.</span> <span class="toc-text">Adam优化算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F"><span class="toc-number">2.6.</span> <span class="toc-text">学习率衰减</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98"><span class="toc-number">2.7.</span> <span class="toc-text">局部最优</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81Batch%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E7%A8%8B%E5%BA%8F%E6%A1%86%E6%9E%B6"><span class="toc-number">3.</span> <span class="toc-text">超参数调试、Batch正则化和程序框架</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E8%AF%95%E5%A4%84%E7%90%86"><span class="toc-number">3.1.</span> <span class="toc-text">调试处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E8%B6%85%E5%8F%82%E6%95%B0%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E8%8C%83%E5%9B%B4"><span class="toc-number">3.2.</span> <span class="toc-text">为超参数选择合适的范围</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Batch-Normalization"><span class="toc-number">3.3.</span> <span class="toc-text">Batch Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Softmax%E5%9B%9E%E5%BD%92"><span class="toc-number">3.4.</span> <span class="toc-text">Softmax回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6"><span class="toc-number">3.5.</span> <span class="toc-text">深度学习框架</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TensorFlow"><span class="toc-number">3.6.</span> <span class="toc-text">TensorFlow</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        吴恩达深度学习笔记（1）——超参数调试与优化
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Rarit7</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2018-04-10T05:56:27.000Z" itemprop="datePublished">2018-04-10</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Andrew-Ng/" rel="tag">Andrew Ng</a>, <a class="tag-link-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h2 id="深度学习的实用层面"><a href="#深度学习的实用层面" class="headerlink" title="深度学习的实用层面"></a>深度学习的实用层面</h2><h3 id="训练集-开发集-测试集"><a href="#训练集-开发集-测试集" class="headerlink" title="训练集/开发集/测试集"></a>训练集/开发集/测试集</h3><ul>
<li>在机器学习中，通常是训练集：测试集=7:3或者训练集：开发集：测试集=6:2:2</li>
<li>在大数据时代，开发集和测试集的容量可以有所减小（对于超百万级别的数据，训练集可以占到99.5%）</li>
<li>确保开发集和测试集的数据与训练集来自同一分布</li>
</ul>
<h3 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h3><ul>
<li>高偏差：欠拟合（例如训练集误差15%，开发集误差16%）</li>
<li>高方差：过拟合（例如训练集误差1%，开发集误差11%）</li>
</ul>
<h3 id="训练神经网络的基本方法"><a href="#训练神经网络的基本方法" class="headerlink" title="训练神经网络的基本方法"></a>训练神经网络的基本方法</h3><ul>
<li>高偏差（训练集性能不佳）-&gt; 尝试规模更大的神经网络/花费更多时间训练算法/更先进的优化算法</li>
<li>高方差（开发集性能不佳）-&gt; 尝试获取更多的数据/正则化/其他神经网络模型</li>
</ul>
<h3 id="L-2-正则化"><a href="#L-2-正则化" class="headerlink" title="$L_2$正则化"></a>$L_2$正则化</h3><p>$$J(W,b)=\frac{1}{m}\sum^m_{i=1}\mathcal{L}(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}||W||^2_2$$</p>
<ul>
<li>欧几里得范数：</li>
</ul>
<p>$$||W||<em>2^2=\sum</em>{j=1}^{n_x}W_j^2=W^TW$$</p>
<ul>
<li>弗罗贝尼乌斯范数（<code>Frobenius Norm</code>）：</li>
</ul>
<p>$$||W^{[l]}||^2_F=\sum^{n^{[l-1]}}<em>{i=1}\sum^{n^{[l]}}</em>{j=1}(W_{ij}^{[l]})^2$$</p>
<ul>
<li>正则化参数$\lambda$设置较大，权重矩阵接近于0，消除或者减少许多隐藏单元的影响，简化神经网络，从而减少过拟合</li>
</ul>
<h3 id="Dropout正则化"><a href="#Dropout正则化" class="headerlink" title="Dropout正则化"></a>Dropout正则化</h3><ul>
<li>反向随机失活：随机删除神经网络中的一些单元，简化神经网络，从而减少过拟合</li>
<li><code>Dropout</code>在计算机视觉中使用比较频繁</li>
</ul>
<h3 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h3><ul>
<li>通过翻转、随机剪裁图片、轻微变形等方法增大数据集</li>
<li><code>Early Stopping</code>：随着迭代次数的增加，训练集性能提升同时开发集性能下降，因此在迭代过程中提早停止训练网络</li>
</ul>
<h3 id="归一化输入"><a href="#归一化输入" class="headerlink" title="归一化输入"></a>归一化输入</h3><ul>
<li>零均值化</li>
<li>归一化方差</li>
</ul>
<h3 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h3><ul>
<li>深层网络反向传播对激活函数（例如<code>sigmoid</code>和<code>tanh</code>）求导，梯度更新以指数衰减或递增，造成梯度消失和梯度爆炸</li>
</ul>
<h3 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h3><ul>
<li>用于开发反向传播的求导是否正确</li>
<li>梯度检验不用于训练，只用于debug</li>
<li>梯度检验不能忽略正则项</li>
<li>梯度检验不能和<code>Dropout</code>混用</li>
<li>随机初始化时运用一次梯度检验，反复训练之后再次运用梯度检验</li>
</ul>
<h2 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h2><h3 id="Mini-Batch梯度下降"><a href="#Mini-Batch梯度下降" class="headerlink" title="Mini-Batch梯度下降"></a>Mini-Batch梯度下降</h3><ul>
<li>把一个包含大量样本的训练集拆分成数个小样本（Mini-Batch）的训练集，从而提高训练的效率</li>
<li>当Mini-Batch的大小等于m时，称为Batch梯度下降，缺点是每次迭代的时间过长</li>
<li>当Mini-Batch的大小等于1时，称为随机梯度下降，缺点是失去向量化带来的算法加速，计算效率低下</li>
<li>视情况选择合适的Mini-Batch的大小</li>
<li>若训练集样本少于2000，直接使用Batch梯度下降</li>
<li>典型的Mini-Batch设置为64~512（常为2的次方）</li>
</ul>
<h3 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h3><p>$$v^t=\beta v^{t-1}+(1-\beta)\theta^t$$</p>
<h3 id="Momentum梯度下降"><a href="#Momentum梯度下降" class="headerlink" title="Momentum梯度下降"></a>Momentum梯度下降</h3><p>$$V_{dW} = \beta v_{dW} + (1-\beta)d_W$$<br>$$V_{db} = \beta v_{db} + (1-\beta)d_b$$<br>$$W = W - \alpha v_{dW}, b = b - \alpha v_{db}$$</p>
<ul>
<li>$\beta$一般取$0.9$</li>
</ul>
<h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p>$$S_{dW} = \beta S_{dW} + (1-\beta)dW^2$$<br>$$S_{db} = \beta S_{db} + (1-\beta)db^2$$<br>$$W = W - \alpha \frac{dW}{\sqrt{S_{dW}+\epsilon}}$$<br>$$b = b - \alpha \frac{db}{\sqrt{S_{db}+\epsilon}}$$</p>
<h3 id="Adam优化算法"><a href="#Adam优化算法" class="headerlink" title="Adam优化算法"></a>Adam优化算法</h3><ul>
<li>结合了Momentum和RMSprop再修正其偏差</li>
<li>Momentum</li>
</ul>
<p>$$V_{dW}=\beta_1V_{dW}+(1-\beta_1)dW$$<br>$$V_{db}=\beta_1V_{db}+(1-\beta_1)db$$</p>
<ul>
<li>RMSprop</li>
</ul>
<p>$$S_{dW}=\beta_2S_{dW}+(1-\beta_2)dW^2$$<br>$$S_{db}=\beta_2S_{db} + (1-\beta_2)db^2$$</p>
<ul>
<li>偏差修正</li>
</ul>
<p>$$V^{corrected}<em>{dW} = \frac{V</em>{dW}}{1-\beta_1^t}, V^{corrected}<em>{db} = \frac{V</em>{db}}{1-\beta_1^t}$$<br>$$S^{corrected}<em>{dW} = \frac{S</em>{dW}}{1-\beta_2^t}, S^{corrected}<em>{db} = \frac{S</em>{db}}{1-\beta_2^t}$$<br>$$W = W - \alpha \frac{V^{corrected}<em>{dW}}{\sqrt{S^{corrected}</em>{dW}}+\epsilon}$$<br>$$b = b - \alpha \frac{V^{corrected}<em>{db}}{\sqrt{S^{corrected}</em>{db}}+\epsilon}$$</p>
<ul>
<li>$\beta_1$一般取$0.9$，$\beta_2$一般取$0.999$，$\epsilon$一般取$10^{-8}$</li>
</ul>
<h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h3><ul>
<li>线性衰减：$\alpha = \frac{1}{1 + \text{decay-rate} * \text{epoch-num}} \alpha_0$</li>
<li>其他方法：指数衰减、平方根衰减、离散下降、手动衰减等</li>
</ul>
<h3 id="局部最优"><a href="#局部最优" class="headerlink" title="局部最优"></a>局部最优</h3><ul>
<li>高维空间中更有可能遇到鞍点而不是局部最优点</li>
</ul>
<h2 id="超参数调试、Batch正则化和程序框架"><a href="#超参数调试、Batch正则化和程序框架" class="headerlink" title="超参数调试、Batch正则化和程序框架"></a>超参数调试、Batch正则化和程序框架</h2><h3 id="调试处理"><a href="#调试处理" class="headerlink" title="调试处理"></a>调试处理</h3><ul>
<li>超参数：学习率$\alpha$， $\beta$（Mometum）， $\beta_1$ / $\beta_2$ / $\epsilon$（Adam），神经网络层数，隐含层的神经元数，学习率衰减率，mini-Batch的大小等</li>
<li>最重要的超参数：学习率$\alpha$</li>
<li>次重要：$\beta$（Mometum）、mini-Batch的大小、隐含层的神经元数</li>
<li>再次重要：网络层数、学习率衰减率</li>
<li>Adam的参数基本采用常用设置</li>
</ul>
<h3 id="为超参数选择合适的范围"><a href="#为超参数选择合适的范围" class="headerlink" title="为超参数选择合适的范围"></a>为超参数选择合适的范围</h3><ul>
<li>用对数标尺搜索超参数比随机均匀取值更合理</li>
<li>例如：$\alpha=10^r,r\in[-4,0]$</li>
</ul>
<h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><ul>
<li>归一化也适用于隐含层的$z^{(1)}$~$z^{(m)}$</li>
</ul>
<p>$$\mu=\frac{1}{m}\sum_i{z^{(i)}}$$<br>$$\sigma^2=\frac{1}{m}\sum_i({z^{(i)}-\mu})^2$$<br>$$z_{norm}^{(i)}=\frac{z^{(i)}-\mu}{(\sqrt{\sigma^2+\epsilon}}$$<br>$$\tilde{z}^{(i)}=\gamma z_{norm}^{(i)}+\beta$$</p>
<ul>
<li>其中$\gamma$和$\beta$是模型的学习参数</li>
<li>BN减少了隐含层变量的分布变化数量，同时有轻微的正则化效果</li>
<li>通常用指数加权平均来调整每个mini-batch训练集中的$\mu$和$\sigma$</li>
</ul>
<h3 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h3><ul>
<li>Softmax将逻辑回归推广到C类，输出$X$属于各分类的概率</li>
<li>$t=e^{z^{[l]}}$，$z^{[l]}$是输出层的元素构成的向量</li>
</ul>
<p>$$a^{[l]}=\frac{e^{z^{[l]}}}{\sum_{j=1}^nt_i}$$<br>$$a^{[l]}<em>i=\frac{t_i}{\sum</em>{j=1}^n t_i}$$</p>
<h3 id="深度学习框架"><a href="#深度学习框架" class="headerlink" title="深度学习框架"></a>深度学习框架</h3><ul>
<li>Caffe/Caffe2</li>
<li>CNTK</li>
<li>DL4J</li>
<li>Keras</li>
<li>Lasagne</li>
<li>mxnet</li>
<li>PaddlePaddle</li>
<li>TensorFlow</li>
<li>Theano</li>
<li>Torch</li>
<li>选择框架的标准：易于编程；运行速度；持续开源</li>
</ul>
<h3 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h3><ul>
<li>示例<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设代价函数是J = w**2 - 10*w + 25，w是我们要优化的变量</span></span><br><span class="line">w = tf.Variable(<span class="number">0</span>, dtype=tf.float32)</span><br><span class="line"><span class="comment"># cost = tf.add(tf.add(w**2, tf.mutiply(-10., w)), 25)</span></span><br><span class="line">cost = w**<span class="number">2</span> - <span class="number">10</span>*w + <span class="number">25</span></span><br><span class="line"><span class="comment"># 学习率0.01</span></span><br><span class="line">train = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(cost)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">session = tf.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出w初始化的值0.0</span></span><br><span class="line">session.run(init)</span><br><span class="line"><span class="built_in">print</span>(session.run(w))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出一步梯度下降后w的值0.1</span></span><br><span class="line">session.run(train)</span><br><span class="line"><span class="built_in">print</span>(session.run(w))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代1000次，w接近于最优值5</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">	session.run(train)</span><br><span class="line"><span class="built_in">print</span>(session.run(w))</span><br></pre></td></tr></table></figure></li>
<li>placeholder()函数<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># placeholder()告诉tensorFlow稍后要为x输入变量</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">cost = x[<span class="number">0</span>][<span class="number">0</span>]*w**<span class="number">2</span> + x[<span class="number">1</span>][<span class="number">0</span>]*w + x[<span class="number">2</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">coefficients = np.array([[<span class="number">1.</span>], [-<span class="number">10.</span>], [<span class="number">25.</span>]])</span><br><span class="line"></span><br><span class="line">session.run(train, feed_dict=[x:coefficients])</span><br></pre></td></tr></table></figure></li>
</ul>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2"><span class="toc-number">1.</span> <span class="toc-text">深度学习的实用层面</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86-%E5%BC%80%E5%8F%91%E9%9B%86-%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-number">1.1.</span> <span class="toc-text">训练集&#x2F;开发集&#x2F;测试集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="toc-number">1.2.</span> <span class="toc-text">偏差和方差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.</span> <span class="toc-text">训练神经网络的基本方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L-2-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">1.4.</span> <span class="toc-text">$L_2$正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">1.5.</span> <span class="toc-text">Dropout正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">1.6.</span> <span class="toc-text">其他正则化方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E8%BE%93%E5%85%A5"><span class="toc-number">1.7.</span> <span class="toc-text">归一化输入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="toc-number">1.8.</span> <span class="toc-text">梯度消失和梯度爆炸</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C"><span class="toc-number">1.9.</span> <span class="toc-text">梯度检验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96"><span class="toc-number">2.</span> <span class="toc-text">优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Mini-Batch%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.1.</span> <span class="toc-text">Mini-Batch梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87"><span class="toc-number">2.2.</span> <span class="toc-text">指数加权平均</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Momentum%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.3.</span> <span class="toc-text">Momentum梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RMSprop"><span class="toc-number">2.4.</span> <span class="toc-text">RMSprop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adam%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">2.5.</span> <span class="toc-text">Adam优化算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F"><span class="toc-number">2.6.</span> <span class="toc-text">学习率衰减</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98"><span class="toc-number">2.7.</span> <span class="toc-text">局部最优</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81Batch%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E7%A8%8B%E5%BA%8F%E6%A1%86%E6%9E%B6"><span class="toc-number">3.</span> <span class="toc-text">超参数调试、Batch正则化和程序框架</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E8%AF%95%E5%A4%84%E7%90%86"><span class="toc-number">3.1.</span> <span class="toc-text">调试处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E8%B6%85%E5%8F%82%E6%95%B0%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E8%8C%83%E5%9B%B4"><span class="toc-number">3.2.</span> <span class="toc-text">为超参数选择合适的范围</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Batch-Normalization"><span class="toc-number">3.3.</span> <span class="toc-text">Batch Normalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Softmax%E5%9B%9E%E5%BD%92"><span class="toc-number">3.4.</span> <span class="toc-text">Softmax回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6"><span class="toc-number">3.5.</span> <span class="toc-text">深度学习框架</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TensorFlow"><span class="toc-number">3.6.</span> <span class="toc-text">TensorFlow</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://ster.im/ng_01/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://ster.im/ng_01/&text=吴恩达深度学习笔记（1）——超参数调试与优化"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://ster.im/ng_01/&title=吴恩达深度学习笔记（1）——超参数调试与优化"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://ster.im/ng_01/&is_video=false&description=吴恩达深度学习笔记（1）——超参数调试与优化"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=吴恩达深度学习笔记（1）——超参数调试与优化&body=Check out this article: http://ster.im/ng_01/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://ster.im/ng_01/&title=吴恩达深度学习笔记（1）——超参数调试与优化"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://ster.im/ng_01/&title=吴恩达深度学习笔记（1）——超参数调试与优化"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://ster.im/ng_01/&title=吴恩达深度学习笔记（1）——超参数调试与优化"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://ster.im/ng_01/&title=吴恩达深度学习笔记（1）——超参数调试与优化"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://ster.im/ng_01/&name=吴恩达深度学习笔记（1）——超参数调试与优化&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://ster.im/ng_01/&t=吴恩达深度学习笔记（1）——超参数调试与优化"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2012-2021
    Rarit7
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdn.bootcdn.net/ajax/libs/clipboard.js/2.0.8/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->


</body>
</html>
