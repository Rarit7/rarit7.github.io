<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="Python sklearn学习笔记（2）：回归（线性模型，树回归，SVM回归，KNN回归，集成回归模型）">
<meta property="og:type" content="article">
<meta property="og:title" content="Python机器学习库笔记（5）——scikit-learn：回归模型">
<meta property="og:url" content="http://ster.im/py_sklearn_1/index.html">
<meta property="og:site_name" content="Rarit7&#39;s Blog">
<meta property="og:description" content="Python sklearn学习笔记（2）：回归（线性模型，树回归，SVM回归，KNN回归，集成回归模型）">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2018-08-11T11:50:16.000Z">
<meta property="article:modified_time" content="2019-03-20T11:35:57.664Z">
<meta property="article:author" content="Rarit7">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="scikit-learn">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon-192x192.png">
        
      
    
    <!-- title -->
    <title>Python机器学习库笔记（5）——scikit-learn：回归模型</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 5.4.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" "Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="上一篇 " href="/py_sklearn_2/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="下一篇 " href="/py_sklearn_0/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="返回顶部 " href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="分享文章 " href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://ster.im/py_sklearn_1/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://ster.im/py_sklearn_1/&text=Python机器学习库笔记（5）——scikit-learn：回归模型"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://ster.im/py_sklearn_1/&title=Python机器学习库笔记（5）——scikit-learn：回归模型"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://ster.im/py_sklearn_1/&is_video=false&description=Python机器学习库笔记（5）——scikit-learn：回归模型"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Python机器学习库笔记（5）——scikit-learn：回归模型&body=Check out this article: http://ster.im/py_sklearn_1/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://ster.im/py_sklearn_1/&title=Python机器学习库笔记（5）——scikit-learn：回归模型"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://ster.im/py_sklearn_1/&title=Python机器学习库笔记（5）——scikit-learn：回归模型"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://ster.im/py_sklearn_1/&title=Python机器学习库笔记（5）——scikit-learn：回归模型"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://ster.im/py_sklearn_1/&title=Python机器学习库笔记（5）——scikit-learn：回归模型"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://ster.im/py_sklearn_1/&name=Python机器学习库笔记（5）——scikit-learn：回归模型&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://ster.im/py_sklearn_1/&t=Python机器学习库笔记（5）——scikit-learn：回归模型"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">广义线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.1.</span> <span class="toc-text">最小二乘法线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B2%AD%E5%9B%9E%E5%BD%92"><span class="toc-number">1.2.</span> <span class="toc-text">岭回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Lasso%E5%9B%9E%E5%BD%92"><span class="toc-number">1.3.</span> <span class="toc-text">Lasso回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%B9%E6%80%A7%E7%BD%91%E7%BB%9C%E5%9B%9E%E5%BD%92"><span class="toc-number">1.4.</span> <span class="toc-text">弹性网络回归</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%91%E5%9B%9E%E5%BD%92"><span class="toc-number">2.</span> <span class="toc-text">树回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%9B%9E%E5%BD%92"><span class="toc-number">3.</span> <span class="toc-text">支持向量机回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#K%E8%BF%91%E9%82%BB%E5%9B%9E%E5%BD%92"><span class="toc-number">4.</span> <span class="toc-text">K近邻回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%EF%BC%9ABagging"><span class="toc-number">5.</span> <span class="toc-text">集成回归模型：Bagging</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Bagging%E5%9B%9E%E5%BD%92"><span class="toc-number">5.1.</span> <span class="toc-text">Bagging回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%9B%9E%E5%BD%92"><span class="toc-number">5.2.</span> <span class="toc-text">随机森林回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%81%E7%AB%AF%E9%9A%8F%E6%9C%BA%E6%A0%91%E5%9B%9E%E5%BD%92"><span class="toc-number">5.3.</span> <span class="toc-text">极端随机树回归</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%EF%BC%9ABoosting"><span class="toc-number">6.</span> <span class="toc-text">集成回归模型：Boosting</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#AdaBoost%E5%9B%9E%E5%BD%92"><span class="toc-number">6.1.</span> <span class="toc-text">AdaBoost回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-Boosting%E5%9B%9E%E5%BD%92"><span class="toc-number">6.2.</span> <span class="toc-text">Gradient Boosting回归</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Python机器学习库笔记（5）——scikit-learn：回归模型
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Rarit7</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2018-08-11T11:50:16.000Z" itemprop="datePublished">2018-08-11</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Python/" rel="tag">Python</a>, <a class="tag-link-link" href="/tags/scikit-learn/" rel="tag">scikit-learn</a>, <a class="tag-link-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>本文将介绍以下几种回归算法在scikit-learn中的使用方法：</p>
<p>基础模型：</p>
<ul>
<li>线性回归（包含岭回归、Lasso回归、弹性网络回归）</li>
<li>树回归</li>
<li>支持向量机回归</li>
<li>K近邻回归</li>
</ul>
<p>集成模型：</p>
<ul>
<li>随机森林回归</li>
<li>极端随机树回归</li>
<li>AdaBoost回归</li>
<li>Gradient Boosting回归</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试用例：波士顿房价数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入波士顿房价数据集</span></span><br><span class="line">boston = load_boston()</span><br><span class="line">X = boston.data</span><br><span class="line">y = boston.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化</span></span><br><span class="line">X = StandardScaler().fit_transform(X)</span><br><span class="line">y = StandardScaler().fit_transform(y.reshape(-<span class="number">1</span>, <span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割训练集与测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>

<h2 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a>广义线性模型</h2><h3 id="最小二乘法线性回归"><a href="#最小二乘法线性回归" class="headerlink" title="最小二乘法线性回归"></a>最小二乘法线性回归</h3><p>最基本的线性回归法，它接收如下的几个参数：</p>
<ul>
<li>fit_intercept：是否考察截距项b，默认为<code>True</code>。</li>
<li>normalize：是否先对数据进行Z-score标准化，默认为<code>False</code>。</li>
<li>copy_X：默认为<code>True</code>则复制X，否则直接在原X上覆写。</li>
<li>n_jobs：使用的处理器核数，默认<code>None</code>表示单核。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">reg = LinearRegression(fit_intercept=<span class="literal">True</span>, normalize=<span class="literal">False</span>, copy_X=<span class="literal">True</span>, n_jobs=<span class="literal">None</span>)</span><br><span class="line">reg.fit(X_train, y_train)</span><br><span class="line">reg.score(X_test, y_test) <span class="comment">#回归模型score返回的是R方，下同</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 各特征的系数w</span></span><br><span class="line">reg.coef_</span><br><span class="line"><span class="comment"># 截距b</span></span><br><span class="line">reg.intercept_</span><br></pre></td></tr></table></figure>

<h3 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h3><p>带L2正则项的线性回归，相比<code>LinearRegression</code>主要多一个正则项系数$\alpha$的参数。</p>
<p>与<code>Ridge</code>相比，<code>RidgeCV</code>内置了交叉验证，会自动帮我们筛出$\alpha$的最优解，省去了超参数调试的麻烦，因此通常采用后者。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV</span><br><span class="line">reg = RidgeCV(alphas=(<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>), fit_intercept=<span class="literal">True</span>, normalize=<span class="literal">False</span>, </span><br><span class="line">              scoring=<span class="literal">None</span>, cv=<span class="number">5</span>, gcv_mode=<span class="literal">None</span>, store_cv_values=<span class="literal">False</span>)</span><br><span class="line">reg.fit(X_train, y_train)</span><br><span class="line">reg.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正则项系数alpha</span></span><br><span class="line">reg.alpha_</span><br></pre></td></tr></table></figure>

<h3 id="Lasso回归"><a href="#Lasso回归" class="headerlink" title="Lasso回归"></a>Lasso回归</h3><p>带L1正则项的线性回归，常用来估计稀疏参数的高维线性模型。</p>
<p>供有<code>Lasso</code>、<code>LassoCV</code>、<code>LassoLars</code>、<code>LassoLarsCV</code>、<code>LassoLarsIC</code>五种可供选择，带<code>CV</code>的即自动选择最优的正则项系数，带<code>Lars</code>的采用最小角回归法而不带<code>Lars</code>的采用坐标轴下降法进行损失函数优化。<code>LassoLarsIC</code>采用AIC（Akaike信息准则）或BIC（Bayes信息准则）确定正则项系数。在大多数回归任务中，首选<code>LassoCV</code>，次选<code>LassoLarsCV</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line">reg = LassoCV(eps=<span class="number">0.001</span>, n_alphas=<span class="number">100</span>, alphas=<span class="literal">None</span>, fit_intercept=<span class="literal">True</span>, </span><br><span class="line">              normalize=<span class="literal">False</span>, precompute=<span class="string">&quot;auto&quot;</span>, max_iter=<span class="number">1000</span>, tol=<span class="number">0.0001</span>, </span><br><span class="line">              copy_X=<span class="literal">True</span>, cv=<span class="number">5</span>, verbose=<span class="literal">False</span>, n_jobs=<span class="literal">None</span>, </span><br><span class="line">              positive=<span class="literal">False</span>, random_state=<span class="literal">None</span>, selection=<span class="string">&quot;cyclic&quot;</span>)</span><br><span class="line">reg.fit(X_train, y_train)</span><br><span class="line">reg.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<h3 id="弹性网络回归"><a href="#弹性网络回归" class="headerlink" title="弹性网络回归"></a>弹性网络回归</h3><p>同时带有L1和L2正则项的线性回归，使用<code>l1_ratio</code>这一权重参数来分配L1和L2正则项的比重。常用<code>ElasticNetCV</code>，它会自动选择正则项系数和平衡权重。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNetCV</span><br><span class="line">reg = ElasticNetCV(l1_ratio=<span class="number">0.5</span>, eps=<span class="number">0.001</span>, n_alphas=<span class="number">100</span>, alphas=<span class="literal">None</span>, </span><br><span class="line">                   fit_intercept=<span class="literal">True</span>, normalize=<span class="literal">False</span>, precompute=<span class="string">&quot;auto&quot;</span>, </span><br><span class="line">                   max_iter=<span class="number">1000</span>, tol=<span class="number">0.0001</span>, cv=<span class="number">5</span>, copy_X=<span class="literal">True</span>, verbose=<span class="number">0</span>, </span><br><span class="line">                   n_jobs=<span class="literal">None</span>, positive=<span class="literal">False</span>, random_state=<span class="literal">None</span>, selection=<span class="string">&quot;cyclic&quot;</span>)</span><br><span class="line">reg.fit(X_train, y_train)</span><br><span class="line">reg.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<p>其他线性模型，敬请参阅<a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model">官方文档：线性回归</a>。</p>
<h2 id="树回归"><a href="#树回归" class="headerlink" title="树回归"></a>树回归</h2><p>CART用于回归时，参数与分类器类似，它可以接收如下的参数：</p>
<ul>
<li><code>criterion</code>：分枝的标准，默认<code>&quot;mse&quot;</code>为均方差，可选<code>&quot;friedman_mse&quot;</code>（Friedman均方差）或者<code>&quot;mae&quot;</code>（绝对平均误差）。通常采用默认值。</li>
<li><code>splitter</code>：分枝的策略，默认<code>&quot;best&quot;</code>在所有划分点中找出最优的划分点，适合样本量不大的情况。样本量巨大时建议选择<code>&quot;random&quot;</code>，在部分划分点中找局部最优的划分点。</li>
<li><code>max_depth</code>：限制树的最大深度，默认值为<code>None</code>。如果样本和特征很多时可以适当限制树的最大深度。</li>
<li><code>min_samples_split</code>：分割一个节点所需的最小样本数，默认为2，当样本量非常大时可以增加这个值。</li>
<li><code>min_samples_leaf</code>：叶节点上所需的最小样本数，叶节点样本数少于这个值时会被剪枝。默认为1，当样本量非常大时可以增加这个值。</li>
<li><code>min_weight_fraction_leaf</code>：叶节点样本权重和所需的最小值，默认为0即视样本具有相同的权重。</li>
<li><code>max_features</code>：分枝时考虑的特征数量最大值，默认<code>&quot;auto&quot;</code>即该值等于特征数量。可以指定整数或者浮点数（表示占特征总数的比例）。也可选<code>&quot;sqrt&quot;</code>（特征数的开根）、<code>&quot;log2&quot;</code>（特征数的对数）、<code>None</code>（等于特征数）。如果特征数较多可以考虑限制以加快模型拟合。</li>
<li><code>random_state</code>：随机数种子。</li>
<li><code>max_leaf_nodes</code>：叶节点数最大值，默认<code>None</code>不对叶节点数量做限制，如果特征较多可以加以限制。</li>
<li><code>min_impurity_decrease</code>：默认为0.，如果分枝导致不纯度的减少大于等于该值，则节点将被分枝。</li>
<li><code>min_impurity_split</code>：默认为1e-7，如果某节点的不纯度超过这个阈值，则该节会分枝，否则该节点为叶节点。</li>
<li><code>presort</code>：是否对数据进行预排序，以加快寻找最佳分割点。默认为<code>False</code>。当使用小数据集或对深度作限制时，设置为<code>True</code>可能会加速训练，但对于大型数据集则反而会变慢。</li>
</ul>
<p>我们超参数调优的主要对象为<code>max_depth</code>、<code>min_samples_split</code>、<code>min_samples_leaf</code>、<code>max_features</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line">reg = DecisionTreeRegressor(criterion=<span class="string">&quot;mse&quot;</span>, splitter=<span class="string">&quot;best&quot;</span>, max_depth=<span class="literal">None</span>, </span><br><span class="line">                            min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, </span><br><span class="line">                            min_weight_fraction_leaf=<span class="number">0.0</span>, max_features=<span class="literal">None</span>, </span><br><span class="line">                            random_state=<span class="literal">None</span>, max_leaf_nodes=<span class="literal">None</span>, </span><br><span class="line">                            min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=<span class="literal">None</span>, </span><br><span class="line">                            presort=<span class="literal">False</span>)</span><br><span class="line">reg.fit(X_train, y_train)</span><br><span class="line">reg.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<h2 id="支持向量机回归"><a href="#支持向量机回归" class="headerlink" title="支持向量机回归"></a>支持向量机回归</h2><p>部分参数如下：</p>
<ul>
<li><code>kernel</code>：核函数，默认使用<code>&quot;rbf&quot;</code>径向基函数，可选<code>&quot;linear&quot;</code>、<code>&quot;poly&quot;</code>、<code>&quot;sigmoid&quot;</code>、<code>&quot;precomputed&quot;</code>或者一个可调用的函数。</li>
<li><code>degree</code>：多项式核函数的维度d，仅在核函数选择<code>&quot;poly&quot;</code>时有效。默认值为3。</li>
<li><code>gamma</code>：<code>&quot;rbf&quot;</code>、<code>&quot;poly&quot;</code>、<code>&quot;sigmoid&quot;</code>的系数gamma，默认为<code>&quot;auto&quot;</code>，取特征数量的倒数。</li>
<li><code>coef0</code>：核函数中的独立项，仅在核函数选择<code>&quot;poly&quot;</code>、<code>&quot;sigmoid&quot;</code>时有效。默认值为0.0。</li>
<li><code>tol</code>：停止训练的误差精度，默认值为1e-3。</li>
<li><code>C</code>：惩罚系数C，默认值为1.0。</li>
<li><code>max_iter</code>：最大迭代次数，默认为-1即无限制。</li>
</ul>
<p>最重要的两个调参对象是<code>gamma</code>和<code>C</code>。gamma越大，支持向量越少，gamma越小，支持向量越多。C可理解为逻辑回归中正则项系数lambda的倒数，C过大容易过拟合，C过小容易欠拟合。通常采用<a href="http://ster.im/py_sklearn_4/#GridSearchCV%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95">网格搜索法</a>进行调参。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR</span><br><span class="line">reg = SVR(kernel=<span class="string">&quot;rbf&quot;</span>, degree=<span class="number">3</span>, gamma=<span class="string">&quot;auto&quot;</span>, coef0=<span class="number">0.0</span>, </span><br><span class="line">          tol=<span class="number">0.001</span>, C=<span class="number">1.0</span>, epsilon=<span class="number">0.1</span>, shrinking=<span class="literal">True</span>, </span><br><span class="line">          cache_size=<span class="number">200</span>, verbose=<span class="literal">False</span>, max_iter=-<span class="number">1</span>)</span><br><span class="line">reg.fit(X_train, y_train)</span><br><span class="line">reg.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<h2 id="K近邻回归"><a href="#K近邻回归" class="headerlink" title="K近邻回归"></a>K近邻回归</h2><p>部分参数如下：</p>
<ul>
<li><code>n_neighbors</code>：最近邻单元的个数K。</li>
<li><code>weights</code>：是否考虑邻居的权重，默认值<code>&quot;uniform&quot;</code>视每个邻居的权重相等，<code>&quot;distance&quot;</code>则给较近的单元更大的权重（取距离的倒数），也可以指定一个可调用的函数。</li>
<li><code>algorithm</code>：计算最近邻的算法，默认<code>&quot;auto&quot;</code>自动挑选模型认为最合适的，可选<code>&quot;ball_tree&quot;</code>、<code>&quot;kd_tree&quot;</code>、<code>&quot;brute&quot;</code>。</li>
<li><code>leaf_size</code>：叶节点数量，默认值30，只有在<code>algorithm</code>选择球树或者KD树时有效。</li>
<li><code>p</code>：闵式距离的度量，p=1时为曼哈顿距离，p=2时为欧式距离（默认）。</li>
</ul>
<p><code>n_neighbors</code>是最需要关注的超参数，其次<code>weights</code>和<code>p</code>也可以适当调整。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor</span><br><span class="line">reg = KNeighborsRegressor(n_neighbors=<span class="number">5</span>, weights=<span class="string">&quot;uniform&quot;</span>, algorithm=<span class="string">&quot;auto&quot;</span>, </span><br><span class="line">                          leaf_size=<span class="number">30</span>, p=<span class="number">2</span>, metric=<span class="string">&quot;minkowski&quot;</span>, metric_params=<span class="literal">None</span>)</span><br><span class="line">reg.fit(X_train, y_train)</span><br><span class="line">reg.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<h2 id="集成回归模型：Bagging"><a href="#集成回归模型：Bagging" class="headerlink" title="集成回归模型：Bagging"></a>集成回归模型：Bagging</h2><h3 id="Bagging回归"><a href="#Bagging回归" class="headerlink" title="Bagging回归"></a>Bagging回归</h3><p>参数：</p>
<ul>
<li><code>base_estimator</code>：基模型，默认<code>None</code>代表决策树，可选择其它基础回归模型对象。</li>
<li><code>n_estimators</code>：基模型的数量，默认为10。</li>
<li><code>max_samples</code>：用于训练基模型的从X_train中抽取样本的数量，可以是整数代表数量，也可以是浮点数代表比例，默认为1.0。</li>
<li><code>max_features</code>：用于训练基模型的从X_train中抽取特征的数量，可以是整数代表数量，也可以是浮点数代表比例，默认为1.0。</li>
<li><code>bootstrap</code>：对于样本是否有放回抽样，默认为<code>True</code>。</li>
<li><code>bootstrap_features</code>：对于特征是否有放回抽样，默认为<code>False</code>。</li>
<li><code>oob_score</code>：是否使用包外样本估计泛化误差。</li>
<li><code>warm_start</code>：默认为<code>False</code>，如果选择<code>True</code>，下一次训练以上一次模型的参数为初始参数。</li>
</ul>
<p>对于所有的集成模型，最需要关注的超参数是<code>n_estimators</code>，即基模型的数量，通常需要使用网格搜索法寻找最优解；其他的参数通常保持默认即可取得较好的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingRegressor</span><br><span class="line">reg = BaggingRegressor(base_estimator=<span class="literal">None</span>, n_estimators=<span class="number">10</span>, max_samples=<span class="number">1.0</span>, </span><br><span class="line">                       max_features=<span class="number">1.0</span>, bootstrap=<span class="literal">True</span>, bootstrap_features=<span class="literal">False</span>, </span><br><span class="line">                       oob_score=<span class="literal">False</span>, warm_start=<span class="literal">False</span>, random_state=<span class="literal">None</span>, verbose=<span class="number">0</span>)</span><br><span class="line">reg.fit(X_train, y_train)</span><br><span class="line">reg.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<h3 id="随机森林回归"><a href="#随机森林回归" class="headerlink" title="随机森林回归"></a>随机森林回归</h3><p>参数：</p>
<ul>
<li><code>n_estimators</code>：树的数量，默认为10。</li>
<li><code>criterion</code>：分枝的标准，默认<code>&quot;mse&quot;</code>为均方差，可选<code>&quot;mae&quot;</code>（绝对平均误差）。</li>
<li><code>max_depth</code>：限制树的最大深度，默认值为<code>None</code>，表示一直分枝直到所有叶节点都是纯的，或者所有叶节点的样本数小于<code>min_samples_split</code>。</li>
<li><code>min_samples_split</code>：分割一个节点所需的最小样本数，默认为2。</li>
<li><code>min_samples_leaf</code>：叶节点上所需的最小样本数，叶节点样本数少于这个值时会被剪枝。默认为1。</li>
<li><code>min_weight_fraction_leaf</code>：叶节点样本权重和所需的最小值，默认为0即视样本具有相同的权重。</li>
<li><code>max_features</code>：分枝时考虑的特征数量最大值，默认<code>&quot;auto&quot;</code>即该值等于特征数量。可以指定整数或者浮点数（表示占特征总数的比例）。也可选<code>&quot;sqrt&quot;</code>（特征数的开根）、<code>&quot;log2&quot;</code>（特征数的对数）、<code>None</code>（等于特征数）。</li>
<li><code>max_leaf_nodes</code>：叶节点数最大值，默认<code>None</code>不对叶节点数量做限制。</li>
<li><code>min_impurity_decrease</code>：默认为0，如果分枝导致不纯度的减少大于等于该值，则节点将被分枝。</li>
<li><code>min_impurity_split</code>：默认为1e-7，如果某节点的不纯度超过这个阈值，则该节会分枝，否则该节点为叶节点。</li>
<li><code>bootstrap</code>：对于样本是否有放回抽样，默认为<code>True</code>。如果为<code>False</code>，则使用整个数据集构建每个树。</li>
<li><code>oob_score</code>：是否使用包外样本估计R方。默认为<code>False</code>。</li>
<li><code>random_state</code>：随机数种子。</li>
<li><code>warm_start</code>：默认为<code>False</code>，如果选择<code>True</code>，下一次训练以上一次模型的参数为初始参数。</li>
</ul>
<p>除了<code>n_estimators</code>之外，还可以考虑适当调整<code>max_depth</code>、<code>min_samples_split</code>、<code>min_samples_leaf</code>、<code>max_features</code>这些决策树的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line">reg = RandomForestRegressor(n_estimators=<span class="number">10</span>, criterion=<span class="string">&quot;mse&quot;</span>, max_depth=<span class="literal">None</span>, </span><br><span class="line">                            min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, </span><br><span class="line">                            min_weight_fraction_leaf=<span class="number">0.0</span>, max_features=<span class="string">&quot;auto&quot;</span>, </span><br><span class="line">                            max_leaf_nodes=<span class="literal">None</span>, min_impurity_decrease=<span class="number">0.0</span>, </span><br><span class="line">                            min_impurity_split=<span class="literal">None</span>, bootstrap=<span class="literal">True</span>, oob_score=<span class="literal">False</span>, </span><br><span class="line">                            random_state=<span class="literal">None</span>, verbose=<span class="number">0</span>, warm_start=<span class="literal">False</span>)</span><br><span class="line">reg.fit(X_train, y_train)</span><br><span class="line">reg.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 各特征的重要性</span></span><br><span class="line">reg.feature_importances_</span><br></pre></td></tr></table></figure>

<h3 id="极端随机树回归"><a href="#极端随机树回归" class="headerlink" title="极端随机树回归"></a>极端随机树回归</h3><p>Extra Tree和随机森林的区别较小，参数几乎一致。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesRegressor</span><br><span class="line">reg = ExtraTreesRegressor(n_estimators=<span class="number">10</span>, criterion=<span class="string">&quot;mse&quot;</span>, max_depth=<span class="literal">None</span>, </span><br><span class="line">                          min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, </span><br><span class="line">                          min_weight_fraction_leaf=<span class="number">0.0</span>, max_features=<span class="string">&quot;auto&quot;</span>, </span><br><span class="line">                          max_leaf_nodes=<span class="literal">None</span>, min_impurity_decrease=<span class="number">0.0</span>, </span><br><span class="line">                          min_impurity_split=<span class="literal">None</span>, bootstrap=<span class="literal">False</span>, oob_score=<span class="literal">False</span>, </span><br><span class="line">                          random_state=<span class="literal">None</span>, verbose=<span class="number">0</span>, warm_start=<span class="literal">False</span>)</span><br><span class="line">reg.fit(X_train, y_train)</span><br><span class="line">reg.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<h2 id="集成回归模型：Boosting"><a href="#集成回归模型：Boosting" class="headerlink" title="集成回归模型：Boosting"></a>集成回归模型：Boosting</h2><h3 id="AdaBoost回归"><a href="#AdaBoost回归" class="headerlink" title="AdaBoost回归"></a>AdaBoost回归</h3><p>参数：</p>
<ul>
<li><code>base_estimator</code>：弱回归学习器，可指定为任意回归模型对象，默认为<code>None</code>，即<code>DecisionTreeRegressor</code>（<code>max_depth=3</code>）。</li>
<li><code>n_estimators</code>：最大迭代次数，即弱学习器的最大个数，默认为50。</li>
<li><code>learning_rate</code>：每个弱学习器的权重缩减系数，介于0.和1.之间，默认为1.。</li>
<li><code>loss</code>：每次迭代后更新权重时采用的损失函数，默认为<code>&quot;linear&quot;</code>，可选<code>&quot;square&quot;</code>、<code>&quot;exponential&quot;</code>，通常使用默认值。</li>
<li><code>random_state</code>：随机数种子。</li>
</ul>
<p><code>n_estimators</code>和<code>learning_rate</code>两个参数相互牵制，通常会一起进行调参。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostRegressor</span><br><span class="line">reg = AdaBoostRegressor(base_estimator=<span class="literal">None</span>, n_estimators=<span class="number">50</span>, learning_rate=<span class="number">1.0</span>, loss=<span class="string">&quot;linear&quot;</span>)</span><br><span class="line">reg.fit(X_train, y_train)</span><br><span class="line">reg.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<h3 id="Gradient-Boosting回归"><a href="#Gradient-Boosting回归" class="headerlink" title="Gradient Boosting回归"></a>Gradient Boosting回归</h3><p>其中决策树部分的参数不列举。</p>
<ul>
<li><code>loss</code>：损失函数，默认值<code>&quot;ls&quot;</code>代表最小二乘回归，可选<code>&quot;lad&quot;</code>（最小绝对偏差）、<code>&quot;huber&quot;</code>（前两者的结合）和<code>&quot;quantile&quot;</code>（分位数回归）。</li>
<li><code>learning_rate</code>：每棵树的权重缩减系数，默认为0.1，与<code>n_estimators</code>相互牵制，是调参的重点。</li>
<li><code>n_estimators</code>：最大迭代次数，默认为100。</li>
<li><code>subsample</code>：子采样率，用于训练每棵树的样本占样本总数的比例，默认为1.0，如使用小于1.0的值，该模型就为随机梯度提升，会减少方差、增大偏差。</li>
<li><code>init</code>：默认为<code>None</code>，可指定具有<code>fit</code>和<code>predict</code>方法的预测器对象，它用于初始化参数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line">reg = GradientBoostingRegressor(loss=<span class="string">&quot;ls&quot;</span>, learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">100</span>, </span><br><span class="line">                                subsample=<span class="number">1.0</span>, criterion=<span class="string">&quot;friedman_mse&quot;</span>, min_samples_split=<span class="number">2</span>, </span><br><span class="line">                                min_samples_leaf=<span class="number">1</span>, min_weight_fraction_leaf=<span class="number">0.0</span>, max_depth=<span class="number">3</span>, </span><br><span class="line">                                min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=<span class="literal">None</span>, init=<span class="literal">None</span>, </span><br><span class="line">                                random_state=<span class="literal">None</span>, max_features=<span class="literal">None</span>, alpha=<span class="number">0.9</span>, verbose=<span class="number">0</span>, </span><br><span class="line">                                max_leaf_nodes=<span class="literal">None</span>, warm_start=<span class="literal">False</span>, presort=<span class="string">&quot;auto&quot;</span>, </span><br><span class="line">                                validation_fraction=<span class="number">0.1</span>, n_iter_no_change=<span class="literal">None</span>, tol=<span class="number">0.0001</span>)</span><br><span class="line">reg.fit(X_train, y_train)</span><br><span class="line">reg.score(X_test, y_test)</span><br></pre></td></tr></table></figure>
  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">广义线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">1.1.</span> <span class="toc-text">最小二乘法线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B2%AD%E5%9B%9E%E5%BD%92"><span class="toc-number">1.2.</span> <span class="toc-text">岭回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Lasso%E5%9B%9E%E5%BD%92"><span class="toc-number">1.3.</span> <span class="toc-text">Lasso回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%B9%E6%80%A7%E7%BD%91%E7%BB%9C%E5%9B%9E%E5%BD%92"><span class="toc-number">1.4.</span> <span class="toc-text">弹性网络回归</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%91%E5%9B%9E%E5%BD%92"><span class="toc-number">2.</span> <span class="toc-text">树回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E5%9B%9E%E5%BD%92"><span class="toc-number">3.</span> <span class="toc-text">支持向量机回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#K%E8%BF%91%E9%82%BB%E5%9B%9E%E5%BD%92"><span class="toc-number">4.</span> <span class="toc-text">K近邻回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%EF%BC%9ABagging"><span class="toc-number">5.</span> <span class="toc-text">集成回归模型：Bagging</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Bagging%E5%9B%9E%E5%BD%92"><span class="toc-number">5.1.</span> <span class="toc-text">Bagging回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%9B%9E%E5%BD%92"><span class="toc-number">5.2.</span> <span class="toc-text">随机森林回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%81%E7%AB%AF%E9%9A%8F%E6%9C%BA%E6%A0%91%E5%9B%9E%E5%BD%92"><span class="toc-number">5.3.</span> <span class="toc-text">极端随机树回归</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%EF%BC%9ABoosting"><span class="toc-number">6.</span> <span class="toc-text">集成回归模型：Boosting</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#AdaBoost%E5%9B%9E%E5%BD%92"><span class="toc-number">6.1.</span> <span class="toc-text">AdaBoost回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-Boosting%E5%9B%9E%E5%BD%92"><span class="toc-number">6.2.</span> <span class="toc-text">Gradient Boosting回归</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://ster.im/py_sklearn_1/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://ster.im/py_sklearn_1/&text=Python机器学习库笔记（5）——scikit-learn：回归模型"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://ster.im/py_sklearn_1/&title=Python机器学习库笔记（5）——scikit-learn：回归模型"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://ster.im/py_sklearn_1/&is_video=false&description=Python机器学习库笔记（5）——scikit-learn：回归模型"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Python机器学习库笔记（5）——scikit-learn：回归模型&body=Check out this article: http://ster.im/py_sklearn_1/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://ster.im/py_sklearn_1/&title=Python机器学习库笔记（5）——scikit-learn：回归模型"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://ster.im/py_sklearn_1/&title=Python机器学习库笔记（5）——scikit-learn：回归模型"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://ster.im/py_sklearn_1/&title=Python机器学习库笔记（5）——scikit-learn：回归模型"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://ster.im/py_sklearn_1/&title=Python机器学习库笔记（5）——scikit-learn：回归模型"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://ster.im/py_sklearn_1/&name=Python机器学习库笔记（5）——scikit-learn：回归模型&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://ster.im/py_sklearn_1/&t=Python机器学习库笔记（5）——scikit-learn：回归模型"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2012-2021
    Rarit7
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdn.bootcdn.net/ajax/libs/clipboard.js/2.0.8/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->


</body>
</html>
