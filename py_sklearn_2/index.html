<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="Python sklearn学习笔记（3）：分类（逻辑回归，决策树，贝叶斯分类器，神经网络，SVM，KNN，集成分类模型）">
<meta property="og:type" content="article">
<meta property="og:title" content="Python机器学习库笔记（6）——scikit-learn：分类模型">
<meta property="og:url" content="https://ster.im/py_sklearn_2/index.html">
<meta property="og:site_name" content="Rarit7&#39;s Blog">
<meta property="og:description" content="Python sklearn学习笔记（3）：分类（逻辑回归，决策树，贝叶斯分类器，神经网络，SVM，KNN，集成分类模型）">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2018-08-19T13:06:04.000Z">
<meta property="article:modified_time" content="2019-03-20T11:37:06.276Z">
<meta property="article:author" content="Rarit7">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="scikit-learn">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon-192x192.png">
        
      
    
    <!-- title -->
    <title>Python机器学习库笔记（6）——scikit-learn：分类模型</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 5.4.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" "Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="上一篇 " href="/py_sklearn_3/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="下一篇 " href="/py_sklearn_1/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="返回顶部 " href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="分享文章 " href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">上一篇</span>
      <span id="i-next" class="info" style="display:none;">下一篇</span>
      <span id="i-top" class="info" style="display:none;">返回顶部</span>
      <span id="i-share" class="info" style="display:none;">分享文章</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://ster.im/py_sklearn_2/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://ster.im/py_sklearn_2/&text=Python机器学习库笔记（6）——scikit-learn：分类模型"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://ster.im/py_sklearn_2/&title=Python机器学习库笔记（6）——scikit-learn：分类模型"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://ster.im/py_sklearn_2/&is_video=false&description=Python机器学习库笔记（6）——scikit-learn：分类模型"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Python机器学习库笔记（6）——scikit-learn：分类模型&body=Check out this article: https://ster.im/py_sklearn_2/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://ster.im/py_sklearn_2/&title=Python机器学习库笔记（6）——scikit-learn：分类模型"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://ster.im/py_sklearn_2/&title=Python机器学习库笔记（6）——scikit-learn：分类模型"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://ster.im/py_sklearn_2/&title=Python机器学习库笔记（6）——scikit-learn：分类模型"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://ster.im/py_sklearn_2/&title=Python机器学习库笔记（6）——scikit-learn：分类模型"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://ster.im/py_sklearn_2/&name=Python机器学习库笔记（6）——scikit-learn：分类模型&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://ster.im/py_sklearn_2/&t=Python机器学习库笔记（6）——scikit-learn：分类模型"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">1.</span> <span class="toc-text">逻辑回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.</span> <span class="toc-text">神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">3.</span> <span class="toc-text">决策树</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">4.</span> <span class="toc-text">支持向量机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KNN"><span class="toc-number">5.</span> <span class="toc-text">KNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="toc-number">6.</span> <span class="toc-text">朴素贝叶斯</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.1.</span> <span class="toc-text">高斯模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.2.</span> <span class="toc-text">多项式模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%AF%E5%8A%AA%E5%88%A9%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.3.</span> <span class="toc-text">伯努利模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E6%A8%A1%E5%9E%8B%EF%BC%9ABagging"><span class="toc-number">7.</span> <span class="toc-text">集成模型：Bagging</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Bagging"><span class="toc-number">7.1.</span> <span class="toc-text">Bagging</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">7.2.</span> <span class="toc-text">随机森林</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%81%E7%AB%AF%E9%9A%8F%E6%9C%BA%E6%A0%91"><span class="toc-number">7.3.</span> <span class="toc-text">极端随机树</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E6%A8%A1%E5%9E%8B%EF%BC%9ABoosting"><span class="toc-number">8.</span> <span class="toc-text">集成模型：Boosting</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#AdaBoost"><span class="toc-number">8.1.</span> <span class="toc-text">AdaBoost</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-Boosting%E5%9B%9E%E5%BD%92"><span class="toc-number">8.2.</span> <span class="toc-text">Gradient Boosting回归</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Python机器学习库笔记（6）——scikit-learn：分类模型
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Rarit7</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2018-08-19T13:06:04.000Z" itemprop="datePublished">2018-08-19</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/Python/" rel="tag">Python</a>, <a class="tag-link-link" href="/tags/scikit-learn/" rel="tag">scikit-learn</a>, <a class="tag-link-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>本文将介绍以下几种分类算法在scikit-learn中的使用方法：</p>
<p>基础模型：</p>
<ul>
<li>逻辑回归</li>
<li>神经网络</li>
<li>决策树</li>
<li>支持向量机</li>
<li>KNN</li>
<li>朴素贝叶斯</li>
</ul>
<p>集成模型：</p>
<ul>
<li>Bagging</li>
<li>随机森林</li>
<li>极端随机树</li>
<li>AdaBoost</li>
<li>GBDT</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试用例：鸢尾花数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 载入鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化</span></span><br><span class="line">X = StandardScaler().fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割训练集与测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>

<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p><code>LogisticRegressionCV</code>比<code>LogisticRegression</code>多出交叉验证求最佳正则项系数的功能，通常使用前者。其主要参数为：</p>
<ul>
<li><code>Cs</code>：浮点列表或者整型，如果为整型，则在1e-4和1e4间以对数步进取值。如同SVM中的C值，是正则项系数lambda的倒数，C越小，正则项对系数的惩罚性越强。</li>
<li><code>fit_intercept</code>：布尔型，是否考虑截距项。默认为<code>True</code>。</li>
<li><code>cv</code>：交叉验证折数，默认<code>None</code>代表3（0.20版本）。</li>
<li><code>penalty</code>：采用何种正则化，默认<code>&quot;l2&quot;</code>，可选<code>&quot;l1&quot;</code>，但注意使用<code>&quot;newton-cg&quot;</code>、<code>&quot;sag&quot;</code>和<code>&quot;lbfgs&quot;</code>这三种优化算法时仅支持<code>&quot;l2&quot;</code>。</li>
<li><code>scoring</code>：评分函数，默认使用<code>&quot;accuracy&quot;</code>准确度，详见<a href="">《SKlearn模型评估》</a>。</li>
<li><code>solver</code>：优化算法，可选<code>&quot;newton-cg&quot;</code>、<code>&quot;lbfgs&quot;</code>（默认）、<code>&quot;liblinear&quot;</code>、<code>&quot;sag&quot;</code>、<code>&quot;saga&quot;</code>。对于小数据集可选<code>&quot;liblinear&quot;</code>，巨型数据集选择随机梯度下降<code>&quot;sag&quot;</code>或<code>&quot;saga&quot;</code>更快；此外，进行多分类任务尽量不选择<code>&quot;liblinear&quot;</code>，因为其只能采用一对多的分类方式。</li>
<li><code>max_iter</code>：优化算法的最大迭代次数。</li>
<li><code>class_weight</code>：类别权重，默认视所有类别具有相同的权重，可选<code>&quot;balanced&quot;</code>自动按照类别频率分配权重，也可指定一个字典。</li>
<li><code>multi_class</code>：多分类时的分类策略，可选<code>&quot;ovr&quot;</code>（默认）、<code>&quot;multinomial&quot;</code>、<code>&quot;auto&quot;</code>。<code>&quot;ovr&quot;</code>即一对多，迭代快、准确性不如多对多；<code>&quot;multinomial&quot;</code>为多对多，迭代慢、准确度高。当优化算法使用<code>&quot;liblinear&quot;</code>时无法使用<code>&quot;multinomial&quot;</code>。</li>
<li><code>random_state</code>：随机数种子。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegressionCV</span><br><span class="line">clf = LogisticRegressionCV(Cs=<span class="number">10</span>, fit_intercept=<span class="literal">True</span>, cv=<span class="number">5</span>, dual=<span class="literal">False</span>, penalty=<span class="string">&quot;l2&quot;</span>, scoring=<span class="literal">None</span>, </span><br><span class="line">                           solver=<span class="string">&quot;lbfgs&quot;</span>, tol=<span class="number">0.0001</span>, max_iter=<span class="number">200</span>, class_weight=<span class="literal">None</span>, n_jobs=<span class="literal">None</span>, </span><br><span class="line">                           verbose=<span class="number">0</span>, refit=<span class="literal">True</span>, multi_class=<span class="string">&quot;multinomial&quot;</span>, random_state=<span class="literal">None</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>scikit-learn中的多层感知机，即全连接神经网络。其具有如下参数：</p>
<ul>
<li><code>hidden_layer_sizes</code>：一个元组，代表每个隐藏层的神经元数，默认为(100,)。</li>
<li><code>activation</code>：激活函数，可选<code>&quot;identity&quot;</code>、<code>&quot;logistic&quot;</code>、<code>&quot;tanh&quot;</code>、<code>&quot;relu&quot;</code>（默认）。</li>
<li><code>solver</code>：优化算法，可选<code>&quot;lbfgs&quot;</code>、<code>&quot;sgd&quot;</code>、<code>&quot;adam&quot;</code>（默认）。默认的<code>&quot;adam&quot;</code>算法适合处理大数据集，而对于小数据集采用<code>&quot;lbfgs&quot;</code>可以更快收敛。</li>
<li><code>alpha</code>：L2正则项系数，默认为0.0001。</li>
<li><code>batch_size</code>：每个batch的大小，如果优化算法选择<code>&quot;lbfgs&quot;</code>将不使用batch。默认<code>&quot;auto&quot;</code>取200和样本总量间的最小值。</li>
<li><code>learning_rate</code>：优化算法选择<code>&quot;sgd&quot;</code>时有效，该参数用于控制学习率的衰减，默认<code>&quot;constant&quot;</code>采用恒定学习率，<code>&quot;invscaling&quot;</code>使用指数衰减，<code>&quot;adaptive&quot;</code>使用自适应学习率衰减。</li>
<li><code>learning_rate_init</code>：优化算法选择<code>&quot;sgd&quot;</code>或<code>&quot;adam&quot;</code>时的初始学习率。默认为0.001。</li>
<li><code>power_t</code>：优化算法选择<code>&quot;sgd&quot;</code>、学习率使用<code>&quot;invscaling&quot;</code>衰减时有效，用于设定迭代次数的指数。默认为0.5。</li>
<li><code>max_iter</code>：最大迭代次数，默认为200，当损失函数低于<code>tol</code>或者迭代次数到达这个值时模型停止迭代。对于<code>&quot;sgd&quot;</code>和<code>&quot;adam&quot;</code>算法，该迭代次数是指整个数据集的迭代次数（epoch），而不是batch的迭代次数。</li>
<li><code>shuffle</code>：仅在选择<code>&quot;sgd&quot;</code>和<code>&quot;adam&quot;</code>算法时有效，是否在每次迭代前对数据洗牌。默认为<code>True</code>。</li>
<li><code>random_state</code>：随机数种子。</li>
<li><code>tol</code>：容差，默认为1e-4，当学习率衰减不设为<code>&quot;adaptive&quot;</code>时，若连续<code>n_iter_no_change</code>次迭代造成的损失函数或验证得分提高低于这个值，则停止迭代。</li>
<li><code>verbose</code>：是否在标准输出中显示进度，默认为<code>False</code>。</li>
<li><code>warm_start</code>：是否热启动，默认为<code>False</code>，当选择<code>True</code>时会使用上一次拟合的参数作为初始参数。</li>
<li><code>momentum</code>：仅在选择<code>&quot;sgd&quot;</code>算法时有效。Momentum梯度下降的动量参数，在0.和1.之间，默认0.9。</li>
<li><code>nesterovs_momentum</code>：仅在选择<code>&quot;sgd&quot;</code>算法、<code>momentum</code>大于0时有效。是否使用Nesterov Momentum，默认为<code>True</code>。</li>
<li><code>early_stopping</code>：仅在选择<code>&quot;sgd&quot;</code>和<code>&quot;adam&quot;</code>算法时有效，当验证分数没有提高时是否提前终止训练。若选择<code>True</code>，它将自动留出10%的训练数据作为验证，并在连续<code>n_iter_no_change</code>次的验证分数没有至少提高<code>tol</code>时终止训练。默认为<code>False</code>。</li>
<li><code>validation_fraction</code>：<code>early_stopping</code>为<code>True</code>时有效，预留用于验证的训练数据的比例，默认0.1。</li>
<li><code>beta_1</code>：仅在选择<code>&quot;adam&quot;</code>算法时有效，adam算法中的一阶矩估计的指数衰减率beta1，默认值0.9。</li>
<li><code>beta_2</code>：仅在选择<code>&quot;adam&quot;</code>算法时有效，adam算法中的二阶矩估计的指数衰减率beta2，默认值0.999。</li>
<li><code>epsilon</code>：仅在选择<code>&quot;adam&quot;</code>算法时有效，adam算法中的epsilon，默认值1e-8。</li>
<li><code>n_iter_no_change</code>：仅在选择<code>&quot;sgd&quot;</code>和<code>&quot;adam&quot;</code>算法时有效，连续<code>n_iter_no_change</code>次迭代造成的损失函数或验证得分提高低于<code>tol</code>，则停止迭代。默认为10。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line">clf = MLPClassifier(hidden_layer_sizes=(<span class="number">100</span>, ), activation=<span class="string">&quot;relu&quot;</span>, solver=<span class="string">&quot;adam&quot;</span>, alpha=<span class="number">0.0001</span>, </span><br><span class="line">                    batch_size=<span class="string">&quot;auto&quot;</span>, learning_rate=<span class="string">&quot;constant&quot;</span>, learning_rate_init=<span class="number">0.001</span>, </span><br><span class="line">                    power_t=<span class="number">0.5</span>, max_iter=<span class="number">1000</span>, shuffle=<span class="literal">True</span>, random_state=<span class="literal">None</span>, tol=<span class="number">0.0001</span>, </span><br><span class="line">                    verbose=<span class="literal">False</span>, warm_start=<span class="literal">False</span>, momentum=<span class="number">0.9</span>, nesterovs_momentum=<span class="literal">True</span>, </span><br><span class="line">                    early_stopping=<span class="literal">False</span>, validation_fraction=<span class="number">0.1</span>, beta_1=<span class="number">0.9</span>, beta_2=<span class="number">0.999</span>, </span><br><span class="line">                    epsilon=<span class="number">1e-08</span>, n_iter_no_change=<span class="number">10</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>CART用于分类，其参数与CART回归类似：</p>
<ul>
<li><code>criterion</code>：分枝的标准，默认<code>&quot;gini&quot;</code>为基尼不纯度，可选<code>&quot;entropy&quot;</code>信息增益。</li>
<li><code>splitter</code>：分枝的策略，默认<code>&quot;best&quot;</code>在所有划分点中找出最优的划分点，适合样本量不大的情况。样本量巨大时建议选择<code>&quot;random&quot;</code>，在部分划分点中找局部最优的划分点。</li>
<li><code>max_depth</code>：限制树的最大深度，默认值为<code>None</code>，即分割至所有叶节点都是纯的或者少于<code>min_samples_split</code>个样本。如果样本和特征很多时可以适当限制树的最大深度。</li>
<li><code>min_samples_split</code>：分割一个节点所需的最小样本数，默认为2，当样本量非常大时可以增加这个值。</li>
<li><code>min_samples_leaf</code>：叶节点上所需的最小样本数，当某个叶节点样本数少于这个值时会被剪枝。默认为1，当样本量非常大时可以增加这个值。</li>
<li><code>min_weight_fraction_leaf</code>：叶节点样本权重和所需的最小值，默认为0即视样本具有相同的权重。</li>
<li><code>max_features</code>：分枝时考虑的特征数量最大值，默认<code>None</code>即该值等于特征数量。可以指定整数或者浮点数（表示占特征总数的比例）。也可选<code>&quot;sqrt&quot;</code>（特征数的开根）、<code>&quot;auto&quot;</code>（同前）、<code>&quot;log2&quot;</code>（特征数的以2为底的对数）。如果特征数较多可以考虑限制以加快模型拟合。</li>
<li><code>random_state</code>：随机数种子。</li>
<li><code>max_leaf_nodes</code>：叶节点数量最大值，默认<code>None</code>不对叶节点数量做限制，如果特征较多可以加以限制。</li>
<li><code>min_impurity_decrease</code>：默认为0.，如果分枝导致不纯度的减少大于等于该值，则节点将被分枝。</li>
<li><code>min_impurity_split</code>：默认为1e-7，如果某节点的不纯度超过这个阈值，则该节会分枝，否则该节点为叶节点。</li>
<li><code>class_weight</code>：接收字典或字典的列表来指定各类别的的权重，也可指定为<code>&quot;balanced&quot;</code>，使用类别出现频率的倒数作为权重。使用默认的<code>None</code>将视所有类别具有相同的权重。</li>
<li><code>presort</code>：是否对数据进行预排序，以加快寻找最佳分割点。默认为<code>False</code>。当使用小数据集或对深度作限制时，设置为<code>True</code>可能会加速训练，但对于大型数据集则反而会变慢。</li>
</ul>
<p>调参的主要对象为<code>max_depth</code>、<code>min_samples_split</code>、<code>min_samples_leaf</code>、<code>max_features</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">clf = DecisionTreeClassifier(criterion=<span class="string">&quot;gini&quot;</span>, splitter=<span class="string">&quot;best&quot;</span>, max_depth=<span class="literal">None</span>, min_samples_split=<span class="number">2</span>, </span><br><span class="line">                             min_samples_leaf=<span class="number">1</span>, min_weight_fraction_leaf=<span class="number">0.0</span>, max_features=<span class="literal">None</span>, </span><br><span class="line">                             random_state=<span class="literal">None</span>, max_leaf_nodes=<span class="literal">None</span>, min_impurity_decrease=<span class="number">0.0</span>, </span><br><span class="line">                             min_impurity_split=<span class="literal">None</span>, class_weight=<span class="literal">None</span>, presort=<span class="literal">False</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><p>部分参数如下：</p>
<ul>
<li><code>C</code>：惩罚系数C，默认值为1.0。</li>
<li><code>kernel</code>：核函数，默认使用<code>&quot;rbf&quot;</code>径向基函数，可选<code>&quot;linear&quot;</code>、<code>&quot;poly&quot;</code>、<code>&quot;sigmoid&quot;</code>、<code>&quot;precomputed&quot;</code>或者一个可调用的函数。</li>
<li><code>degree</code>：多项式核函数的维度d，仅在核函数选择<code>&quot;poly&quot;</code>时有效。默认值为3。</li>
<li><code>gamma</code>：<code>&quot;rbf&quot;</code>、<code>&quot;poly&quot;</code>、<code>&quot;sigmoid&quot;</code>的系数gamma，默认为<code>&quot;auto&quot;</code>，取特征数量的倒数，如果使用<code>&quot;scale&quot;</code>，则取特征数量乘以变量二阶矩再取倒数。</li>
<li><code>coef0</code>：核函数中的独立项，仅在核函数选择<code>&quot;poly&quot;</code>、<code>&quot;sigmoid&quot;</code>时有效。默认值为0.0。</li>
<li><code>shrinking</code>：是否使用shrinking heuristic方法，默认为<code>True</code>。</li>
<li><code>probability</code>：是否使用概率估计，默认为<code>False</code>。</li>
<li><code>tol</code>：停止训练的误差精度，默认值为1e-3。</li>
<li><code>cache_size</code>：核函数缓存大小。</li>
<li><code>class_weight</code>：接收字典或字典的列表来指定各类别的的权重，也可指定为<code>&quot;balanced&quot;</code>，使用类别出现频率的倒数作为权重。使用默认的<code>None</code>将视所有类别具有相同的权重。</li>
<li><code>max_iter</code>：最大迭代次数，默认为-1即无限制。</li>
<li><code>decision_function_shape</code>：多分类策略，可选<code>&quot;ovo&quot;</code>或<code>&quot;ovr&quot;</code>（默认）。</li>
<li><code>random_state</code>：随机数种子。</li>
</ul>
<p>最重要的两个调参对象是<code>gamma</code>和<code>C</code>。gamma越大，支持向量越少，gamma越小，支持向量越多。C可理解为逻辑回归中正则项系数lambda的倒数，C过大容易过拟合，C过小容易欠拟合。通常采用<a href="http://ster.im/py_sklearn_4/#GridSearchCV%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95">网格搜索法</a>进行调参。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">clf = SVC(C=<span class="number">1.0</span>, kernel=<span class="string">&quot;rbf&quot;</span>, degree=<span class="number">3</span>, gamma=<span class="string">&quot;auto&quot;</span>, coef0=<span class="number">0.0</span>, shrinking=<span class="literal">True</span>, </span><br><span class="line">          probability=<span class="literal">False</span>, tol=<span class="number">0.001</span>, cache_size=<span class="number">200</span>, class_weight=<span class="literal">None</span>, verbose=<span class="literal">False</span>, </span><br><span class="line">          max_iter=-<span class="number">1</span>, decision_function_shape=<span class="string">&quot;ovr&quot;</span>, random_state=<span class="literal">None</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><p>部分参数如下：</p>
<ul>
<li><code>n_neighbors</code>：最近邻单元的个数K，默认为5。</li>
<li><code>weights</code>：是否考虑邻居的权重，默认值<code>&quot;uniform&quot;</code>视每个邻居的权重相等，<code>&quot;distance&quot;</code>则给较近的单元更大的权重（取距离的倒数），也可以指定一个可调用的函数。</li>
<li><code>algorithm</code>：计算最近邻的算法，默认<code>&quot;auto&quot;</code>自动挑选模型认为最合适的，可选<code>&quot;ball_tree&quot;</code>、<code>&quot;kd_tree&quot;</code>、<code>&quot;brute&quot;</code>。</li>
<li><code>leaf_size</code>：叶节点数量，默认值30，只有在<code>algorithm</code>选择球树或者KD树时有效。</li>
<li><code>p</code>：闵式距离的度量，p=1时为曼哈顿距离，p=2时为欧式距离（默认）。</li>
</ul>
<p><code>n_neighbors</code>是最需要关注的超参数，其次<code>weights</code>和<code>p</code>也可以适当调整。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">clf = KNeighborsClassifier(n_neighbors=<span class="number">5</span>, weights=<span class="string">&quot;uniform&quot;</span>, algorithm=<span class="string">&quot;auto&quot;</span>, leaf_size=<span class="number">30</span>, </span><br><span class="line">                           p=<span class="number">2</span>, metric=<span class="string">&quot;minkowski&quot;</span>, metric_params=<span class="literal">None</span>, n_jobs=<span class="literal">None</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>scikit-learn提供了以下三种朴素贝叶斯模型：</p>
<h3 id="高斯模型"><a href="#高斯模型" class="headerlink" title="高斯模型"></a>高斯模型</h3><p>当特征是连续变量时常采用高斯模型。其参数：</p>
<ul>
<li><code>priors</code>：先验概率，如果指定为一个形如(n_classes, )的数组，则不根据数据调整先验概率。</li>
<li><code>var_smoothing</code>：为稳定性而加入的方差，默认为1e-9。</li>
</ul>
<h3 id="多项式模型"><a href="#多项式模型" class="headerlink" title="多项式模型"></a>多项式模型</h3><p>当特征是离散变量时常采用多项式模型。其参数：</p>
<ul>
<li><code>alpha</code>：平滑参数，默认值为1.0。</li>
<li><code>fit_prior</code>：是否要考虑先验概率，如果选择<code>False</code>，对所有类别使用一致的先验概率。</li>
<li><code>class_prior</code>：先验概率，如果指定为一个形如(n_classes, )的数组，则不根据数据调整先验概率。</li>
</ul>
<h3 id="伯努利模型"><a href="#伯努利模型" class="headerlink" title="伯努利模型"></a>伯努利模型</h3><p>当特征是布尔型变量时常采用伯努利模型。其参数：</p>
<ul>
<li><code>alpha</code>：平滑参数，默认值为1.0。</li>
<li><code>binarize</code>：对特征进行二值化的阈值，默认为0.0，如果设为<code>None</code>则假定输入特征已经二值化。</li>
<li><code>fit_prior</code>：是否要考虑先验概率，如果选择<code>False</code>，对所有类别使用一致的先验概率。</li>
<li><code>class_prior</code>：先验概率，如果指定为一个形如(n_classes, )的数组，则不根据数据调整先验概率。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line">clf = GaussianNB(priors=<span class="literal">None</span>, var_smoothing=<span class="number">1e-09</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<h2 id="集成模型：Bagging"><a href="#集成模型：Bagging" class="headerlink" title="集成模型：Bagging"></a>集成模型：Bagging</h2><h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><p>参数：</p>
<ul>
<li><code>base_estimator</code>：基模型，默认<code>None</code>代表决策树，可选择其它基础回归模型对象。</li>
<li><code>n_estimators</code>：基模型的数量，默认为10。</li>
<li><code>max_samples</code>：用于训练基模型的从X_train中抽取样本的数量，可以是整数代表数量，也可以是浮点数代表比例，默认为1.0。</li>
<li><code>max_features</code>：用于训练基模型的从X_train中抽取特征的数量，可以是整数代表数量，也可以是浮点数代表比例，默认为1.0。</li>
<li><code>bootstrap</code>：对于样本是否有放回抽样，默认为<code>True</code>。</li>
<li><code>bootstrap_features</code>：对于特征是否有放回抽样，默认为<code>False</code>。</li>
<li><code>oob_score</code>：是否使用包外样本估计泛化误差。</li>
<li><code>warm_start</code>：默认为<code>False</code>，如果选择<code>True</code>，下一次训练以上一次模型的参数为初始参数。</li>
</ul>
<p>对于所有的集成模型，最需要关注的超参数是<code>n_estimators</code>，即基模型的数量，通常需要使用网格搜索法寻找最优解；其他的参数通常保持默认即可取得较好的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line">clf = BaggingClassifier(base_estimator=<span class="literal">None</span>, n_estimators=<span class="number">10</span>, max_samples=<span class="number">1.0</span>, max_features=<span class="number">1.0</span>, </span><br><span class="line">                        bootstrap=<span class="literal">True</span>, bootstrap_features=<span class="literal">False</span>, oob_score=<span class="literal">False</span>, warm_start=<span class="literal">False</span>, </span><br><span class="line">                        n_jobs=<span class="literal">None</span>, random_state=<span class="literal">None</span>, verbose=<span class="number">0</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><p>参数：</p>
<ul>
<li><code>n_estimators</code>：树的数量，默认为10。</li>
<li><code>criterion</code>：分枝的标准，默认”gini”为基尼不纯度，可选”entropy”信息增益。</li>
<li><code>max_depth</code>：限制树的最大深度，默认值为<code>None</code>，表示一直分枝直到所有叶节点都是纯的，或者所有叶节点的样本数小于<code>min_samples_split</code>。</li>
<li><code>min_samples_split</code>：分割一个节点所需的最小样本数，默认为2。</li>
<li><code>min_samples_leaf</code>：叶节点上所需的最小样本数，叶节点样本数少于这个值时会被剪枝。默认为1。</li>
<li><code>min_weight_fraction_leaf</code>：叶节点样本权重和所需的最小值，默认为0即视样本具有相同的权重。</li>
<li><code>max_features</code>：分枝时考虑的特征数量最大值，默认<code>&quot;auto&quot;</code>相当于<code>&quot;sqrt&quot;</code>。可以指定整数或者浮点数（表示占特征总数的比例）。也可选<code>&quot;sqrt&quot;</code>（特征数的开根）、<code>&quot;log2&quot;</code>（特征数的对数）、<code>None</code>（等于特征数）。</li>
<li><code>max_leaf_nodes</code>：叶节点数最大值，默认<code>None</code>不对叶节点数量做限制。</li>
<li><code>min_impurity_decrease</code>：默认为0，如果分枝导致不纯度的减少大于等于该值，则节点将被分枝。</li>
<li><code>min_impurity_split</code>：默认为1e-7，如果某节点的不纯度超过这个阈值，则该节会分枝，否则该节点为叶节点。</li>
<li><code>bootstrap</code>：对于样本是否有放回抽样，默认为<code>True</code>。如果为<code>False</code>，则使用整个数据集构建每个树。</li>
<li><code>oob_score</code>：是否使用包外样本估计R方。默认为<code>False</code>。</li>
<li><code>random_state</code>：随机数种子。</li>
<li><code>warm_start</code>：默认为<code>False</code>，如果选择<code>True</code>，下一次训练以上一次模型的参数为初始参数。</li>
<li><code>class_weight</code>：接收字典或字典的列表来指定各类别的的权重，也可指定为<code>&quot;balanced&quot;</code>，使用类别出现频率的倒数作为权重；指定为<code>&quot;balanced_subsample&quot;</code>则每棵树使用其抽样样本计算权重。使用默认的<code>None</code>将视所有类别具有相同的权重。</li>
</ul>
<p>除了<code>n_estimators</code>之外，还可以考虑适当调整<code>max_depth</code>、<code>min_samples_split</code>、<code>min_samples_leaf</code>、<code>max_features</code>这些决策树的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">clf = RandomForestClassifier(n_estimators=<span class="number">10</span>, criterion=<span class="string">&quot;gini&quot;</span>, max_depth=<span class="literal">None</span>, min_samples_split=<span class="number">2</span>, </span><br><span class="line">                             min_samples_leaf=<span class="number">1</span>, min_weight_fraction_leaf=<span class="number">0.0</span>, max_features=<span class="string">&quot;auto&quot;</span>, </span><br><span class="line">                             max_leaf_nodes=<span class="literal">None</span>, min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=<span class="literal">None</span>, </span><br><span class="line">                             bootstrap=<span class="literal">True</span>, oob_score=<span class="literal">False</span>, n_jobs=<span class="literal">None</span>, random_state=<span class="literal">None</span>, verbose=<span class="number">0</span>, </span><br><span class="line">                             warm_start=<span class="literal">False</span>, class_weight=<span class="literal">None</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<h3 id="极端随机树"><a href="#极端随机树" class="headerlink" title="极端随机树"></a>极端随机树</h3><p>Extra Tree和随机森林的区别较小，参数几乎一致。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier</span><br><span class="line">clf = ExtraTreesClassifier(n_estimators=<span class="number">10</span>, criterion=<span class="string">&quot;gini&quot;</span>, max_depth=<span class="literal">None</span>, min_samples_split=<span class="number">2</span>, </span><br><span class="line">                           min_samples_leaf=<span class="number">1</span>, min_weight_fraction_leaf=<span class="number">0.0</span>, max_features=<span class="string">&quot;auto&quot;</span>, </span><br><span class="line">                           max_leaf_nodes=<span class="literal">None</span>, min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=<span class="literal">None</span>, </span><br><span class="line">                           bootstrap=<span class="literal">False</span>, oob_score=<span class="literal">False</span>, n_jobs=<span class="literal">None</span>, random_state=<span class="literal">None</span>, </span><br><span class="line">                           verbose=<span class="number">0</span>, warm_start=<span class="literal">False</span>, class_weight=<span class="literal">None</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<h2 id="集成模型：Boosting"><a href="#集成模型：Boosting" class="headerlink" title="集成模型：Boosting"></a>集成模型：Boosting</h2><h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h3><p>参数：</p>
<ul>
<li><code>base_estimator</code>：弱分类器，可指定为任意分类模型对象，默认为<code>None</code>，即<code>DecisionTreeClassifier</code>（<code>max_depth=1</code>）。</li>
<li><code>n_estimators</code>：最大迭代次数，即弱学习器的最大个数，默认为50。</li>
<li><code>learning_rate</code>：每个弱学习器的权重缩减系数，介于0.和1.之间，默认为1.。</li>
<li><code>algorithm</code>：算法，可选<code>&quot;SAMME&quot;</code>, <code>&quot;SAMME.R&quot;</code>。默认使用的SAMME.R算法收敛速度通常比SAMME算法快，在迭代次数较少的情况下能取得更低的测试误差。</li>
<li><code>random_state</code>：随机数种子。</li>
</ul>
<p><code>n_estimators</code>和<code>learning_rate</code>两个参数相互牵制，通常会一起进行调参。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line">clf = AdaBoostClassifier(base_estimator=<span class="literal">None</span>, n_estimators=<span class="number">50</span>, learning_rate=<span class="number">1.0</span>, </span><br><span class="line">                         algorithm=<span class="string">&quot;SAMME.R&quot;</span>, random_state=<span class="literal">None</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<h3 id="Gradient-Boosting回归"><a href="#Gradient-Boosting回归" class="headerlink" title="Gradient Boosting回归"></a>Gradient Boosting回归</h3><p>其中决策树部分的参数不列举。</p>
<ul>
<li><code>loss</code>：损失函数，默认值<code>&quot;deviance&quot;</code>使用对数损失函数，可选<code>&quot;exponential&quot;</code>，它是Adaboost的损失函数。</li>
<li><code>learning_rate</code>：每棵树的权重缩减系数，默认为0.1，与<code>n_estimators</code>相互牵制，是调参的重点。</li>
<li><code>n_estimators</code>：最大迭代次数，默认为100。</li>
<li><code>subsample</code>：子采样率，用于训练每棵树的样本占样本总数的比例，默认为1.0，如使用小于1.0的值，该模型就为随机梯度提升，会减少方差、增大偏差。</li>
<li><code>init</code>：默认为<code>None</code>，可指定具有<code>fit</code>和<code>predict</code>方法的预测器对象，它用于初始化参数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line">clf = GradientBoostingClassifier(loss=<span class="string">&quot;deviance&quot;</span>, learning_rate=<span class="number">0.1</span>, n_estimators=<span class="number">100</span>, subsample=<span class="number">1.0</span>, </span><br><span class="line">                                 criterion=<span class="string">&quot;friedman_mse&quot;</span>, min_samples_split=<span class="number">2</span>, min_samples_leaf=<span class="number">1</span>, </span><br><span class="line">                                 min_weight_fraction_leaf=<span class="number">0.0</span>, max_depth=<span class="number">3</span>, min_impurity_decrease=<span class="number">0.0</span>, </span><br><span class="line">                                 min_impurity_split=<span class="literal">None</span>, init=<span class="literal">None</span>, random_state=<span class="literal">None</span>, max_features=<span class="literal">None</span>, </span><br><span class="line">                                 verbose=<span class="number">0</span>, max_leaf_nodes=<span class="literal">None</span>, warm_start=<span class="literal">False</span>, presort=<span class="string">&quot;auto&quot;</span>, </span><br><span class="line">                                 validation_fraction=<span class="number">0.1</span>, n_iter_no_change=<span class="literal">None</span>, tol=<span class="number">0.0001</span>)</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure>
  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">1.</span> <span class="toc-text">逻辑回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.</span> <span class="toc-text">神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">3.</span> <span class="toc-text">决策树</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">4.</span> <span class="toc-text">支持向量机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#KNN"><span class="toc-number">5.</span> <span class="toc-text">KNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="toc-number">6.</span> <span class="toc-text">朴素贝叶斯</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.1.</span> <span class="toc-text">高斯模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.2.</span> <span class="toc-text">多项式模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%AF%E5%8A%AA%E5%88%A9%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.3.</span> <span class="toc-text">伯努利模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E6%A8%A1%E5%9E%8B%EF%BC%9ABagging"><span class="toc-number">7.</span> <span class="toc-text">集成模型：Bagging</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Bagging"><span class="toc-number">7.1.</span> <span class="toc-text">Bagging</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">7.2.</span> <span class="toc-text">随机森林</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%81%E7%AB%AF%E9%9A%8F%E6%9C%BA%E6%A0%91"><span class="toc-number">7.3.</span> <span class="toc-text">极端随机树</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E6%A8%A1%E5%9E%8B%EF%BC%9ABoosting"><span class="toc-number">8.</span> <span class="toc-text">集成模型：Boosting</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#AdaBoost"><span class="toc-number">8.1.</span> <span class="toc-text">AdaBoost</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-Boosting%E5%9B%9E%E5%BD%92"><span class="toc-number">8.2.</span> <span class="toc-text">Gradient Boosting回归</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://ster.im/py_sklearn_2/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://ster.im/py_sklearn_2/&text=Python机器学习库笔记（6）——scikit-learn：分类模型"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://ster.im/py_sklearn_2/&title=Python机器学习库笔记（6）——scikit-learn：分类模型"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://ster.im/py_sklearn_2/&is_video=false&description=Python机器学习库笔记（6）——scikit-learn：分类模型"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Python机器学习库笔记（6）——scikit-learn：分类模型&body=Check out this article: https://ster.im/py_sklearn_2/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://ster.im/py_sklearn_2/&title=Python机器学习库笔记（6）——scikit-learn：分类模型"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://ster.im/py_sklearn_2/&title=Python机器学习库笔记（6）——scikit-learn：分类模型"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://ster.im/py_sklearn_2/&title=Python机器学习库笔记（6）——scikit-learn：分类模型"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://ster.im/py_sklearn_2/&title=Python机器学习库笔记（6）——scikit-learn：分类模型"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://ster.im/py_sklearn_2/&name=Python机器学习库笔记（6）——scikit-learn：分类模型&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://ster.im/py_sklearn_2/&t=Python机器学习库笔记（6）——scikit-learn：分类模型"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> 菜单</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> 目录</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> 分享</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> 返回顶部</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2012-2021
    Rarit7
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">首页</a></li>
         
          <li><a href="/about/">关于</a></li>
         
          <li><a href="/archives/">归档</a></li>
         
          <li><a href="/search/">搜索</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdn.bootcdn.net/ajax/libs/clipboard.js/2.0.8/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"复制到粘贴板!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "复制成功!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->


</body>
</html>
